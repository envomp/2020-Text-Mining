{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SYUrHxioHQxA"
   },
   "source": [
    "!pip install conllu estnltk==1.4.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "class TextRank4Keyword():\n",
    "    \"\"\"Extract keywords from text\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.d = 0.85 # damping coefficient, usually is .85\n",
    "        self.min_diff = 1e-5 # convergence threshold\n",
    "        self.steps = 10 # iteration steps\n",
    "        self.node_weight = None # save keywords and its weight\n",
    "\n",
    "    \n",
    "    def set_stopwords(self, stopwords):  \n",
    "        \"\"\"Set stop words\"\"\"\n",
    "        for word in STOP_WORDS.union(set(stopwords)):\n",
    "            lexeme = nlp.vocab[word]\n",
    "            lexeme.is_stop = True\n",
    "    \n",
    "    def sentence_segment(self, doc, candidate_pos, lower):\n",
    "        \"\"\"Store those words only in cadidate_pos\"\"\"\n",
    "        sentences = []\n",
    "        for sent in doc.sents:\n",
    "            selected_words = []\n",
    "            for token in sent:\n",
    "                # Store words only with cadidate POS tag\n",
    "                if token.pos_ in candidate_pos and token.is_stop is False:\n",
    "                    if lower is True:\n",
    "                        selected_words.append(token.text.lower())\n",
    "                    else:\n",
    "                        selected_words.append(token.text)\n",
    "            sentences.append(selected_words)\n",
    "        return sentences\n",
    "        \n",
    "    def get_vocab(self, sentences):\n",
    "        \"\"\"Get all tokens\"\"\"\n",
    "        vocab = OrderedDict()\n",
    "        i = 0\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = i\n",
    "                    i += 1\n",
    "        return vocab\n",
    "    \n",
    "    def get_token_pairs(self, window_size, sentences):\n",
    "        \"\"\"Build token_pairs from windows in sentences\"\"\"\n",
    "        token_pairs = list()\n",
    "        for sentence in sentences:\n",
    "            for i, word in enumerate(sentence):\n",
    "                for j in range(i+1, i+window_size):\n",
    "                    if j >= len(sentence):\n",
    "                        break\n",
    "                    pair = (word, sentence[j])\n",
    "                    if pair not in token_pairs:\n",
    "                        token_pairs.append(pair)\n",
    "        return token_pairs\n",
    "        \n",
    "    def symmetrize(self, a):\n",
    "        return a + a.T - np.diag(a.diagonal())\n",
    "    \n",
    "    def get_matrix(self, vocab, token_pairs):\n",
    "        \"\"\"Get normalized matrix\"\"\"\n",
    "        # Build matrix\n",
    "        vocab_size = len(vocab)\n",
    "        g = np.zeros((vocab_size, vocab_size), dtype='float')\n",
    "        for word1, word2 in token_pairs:\n",
    "            i, j = vocab[word1], vocab[word2]\n",
    "            g[i][j] = 1\n",
    "            \n",
    "        # Get Symmeric matrix\n",
    "        g = self.symmetrize(g)\n",
    "        \n",
    "        # Normalize matrix by column\n",
    "        norm = np.sum(g, axis=0)\n",
    "        g_norm = np.divide(g, norm, where=norm!=0) # this is ignore the 0 element in norm\n",
    "        \n",
    "        return g_norm\n",
    "\n",
    "    \n",
    "    def get_keywords(self, number=10):\n",
    "        \"\"\"Print top number keywords\"\"\"\n",
    "        node_weight = OrderedDict(sorted(self.node_weight.items(), key=lambda t: t[1], reverse=True))\n",
    "        return node_weight.items()\n",
    "    \n",
    "        \n",
    "    def analyze(self, text, \n",
    "                candidate_pos=['NOUN', 'PROPN'], \n",
    "                window_size=4, lower=False, stopwords=list()):\n",
    "        \"\"\"Main function to analyze text\"\"\"\n",
    "        \n",
    "        # Set stop words\n",
    "        self.set_stopwords(stopwords)\n",
    "        \n",
    "        # Pare text by spaCy\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        # Filter sentences\n",
    "        sentences = self.sentence_segment(doc, candidate_pos, lower) # list of list of words\n",
    "        \n",
    "        # Build vocabulary\n",
    "        vocab = self.get_vocab(sentences)\n",
    "        \n",
    "        # Get token_pairs from windows\n",
    "        token_pairs = self.get_token_pairs(window_size, sentences)\n",
    "        \n",
    "        # Get normalized matrix\n",
    "        g = self.get_matrix(vocab, token_pairs)\n",
    "        \n",
    "        # Initionlization for weight(pagerank value)\n",
    "        pr = np.array([1] * len(vocab))\n",
    "        \n",
    "        # Iteration\n",
    "        previous_pr = 0\n",
    "        for epoch in range(self.steps):\n",
    "            pr = (1-self.d) + self.d * np.dot(g, pr)\n",
    "            if abs(previous_pr - sum(pr))  < self.min_diff:\n",
    "                break\n",
    "            else:\n",
    "                previous_pr = sum(pr)\n",
    "\n",
    "        # Get weight for each node\n",
    "        node_weight = dict()\n",
    "        for word, index in vocab.items():\n",
    "            node_weight[word] = pr[index]\n",
    "        \n",
    "        self.node_weight = node_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors # to load the model\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format('lemmas.sg.s200.w2v.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(tp, fp):\n",
    "    return tp / (tp + fp)\n",
    "\n",
    "def recall(tp, fn):\n",
    "    return tp / (tp + fn)\n",
    "\n",
    "def f1(p, r):\n",
    "    return 2 * (p * r) / (p + r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_5_keywords = set([\"elurikkus\", \"aasta\", \"omavalitsus\", \"inimene\", \"rohevÃµrgustik\"])\n",
    "real_10_keywords = real_5_keywords | set([\"hoidmine\", \"liik\", \"keskkond\", \"kultuur\", \"planeering\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dxENjZ3yMbBN",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from estnltk import Text\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "import html\n",
    "from collections import Counter\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy\n",
    "import networkx as nx\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "\n",
    "\n",
    "def is_text(word):\n",
    "    return len(set(word[\"analysis\"][0][\"partofspeech\"]) & set(['Z'])) < 1\n",
    "\n",
    "def is_substantive(word):\n",
    "    return len(set(word[\"analysis\"][0][\"partofspeech\"]) & set(['S'])) > 0\n",
    "\n",
    "def is_adjective_or_substantive(word):\n",
    "    return len(set(word[\"analysis\"][0][\"partofspeech\"]) & set(['S', 'A'])) > 0\n",
    "\n",
    "stop_words = [x.strip() for x in open(\"estonian-stopwords-lemmas.txt\", \"r\", encoding=\"utf-8\").readlines()]\n",
    "set_stop_words = set(stop_words)\n",
    "\n",
    "def method_generator(text):\n",
    "    textRank = Counter()\n",
    "    \n",
    "    lemma_text = \"\"\n",
    "    un_stopped_text = []\n",
    "    \n",
    "    for word in text.words:\n",
    "        lemma = word[\"analysis\"][0][\"lemma\"]\n",
    "        lemma_text += lemma + \" \"\n",
    "        if lemma not in stop_words and is_text(word):\n",
    "            un_stopped_text.append(lemma)\n",
    "        \n",
    "        if is_substantive(word):\n",
    "            textRank[lemma] += 1\n",
    "    \n",
    "    tr4w = TextRank4Keyword()\n",
    "    tr4w.analyze(lemma_text, candidate_pos = ['NOUN'], window_size=4, lower=False, stopwords=stop_words)\n",
    "    \n",
    "    LDA_texts = [un_stopped_text]\n",
    "    dictionary = corpora.Dictionary(LDA_texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in LDA_texts]\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=1, id2word = dictionary, passes=20)\n",
    "    \n",
    "    LSA_texts = [un_stopped_text]\n",
    "    dictionary = corpora.Dictionary(LSA_texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in LSA_texts]\n",
    "    lsamodel = gensim.models.LsiModel(corpus, num_topics=1, id2word = dictionary)\n",
    "    \n",
    "    def get_text_rank_words_from_model(model, nr_of_words):\n",
    "        method_res = method.most_common(nr_of_words)\n",
    "        return [x for x, y in method_res]\n",
    "    \n",
    "    def get_words_from_model(model, nr_of_words):\n",
    "        return [x.split('\"')[1] for x in model.print_topics(num_words=nr_of_words)[0][1].split(\"+\")]\n",
    "\n",
    "    sets = [(5, real_5_keywords), (10, real_10_keywords)]\n",
    "    methods = [(\"TextRank\", textRank, get_text_rank_words_from_model),\n",
    "               (\"LDA\", ldamodel, get_words_from_model),\n",
    "               (\"LSA\", lsamodel, get_words_from_model)]\n",
    "    results = []\n",
    "    for count, answer in sets:\n",
    "\n",
    "        print(\"\\tkus otsib \" + str(count) + \" vÃµtmesÃµna:\\n\")\n",
    "        for prefix, method, resolver in methods:\n",
    "            res = resolver(method, count)\n",
    "            results.append(res)\n",
    "            print(\"\\tAndis \" + prefix + \" lÃ¤henemine vÃµtmesÃµnadeks:\\n\\t\" + \", \".join(res))\n",
    "            tp = len(answer & set(res))\n",
    "            fp = len(set(res) - answer)\n",
    "            fn = len(answer - set(res))\n",
    "            res_percision = precision(tp, fp)\n",
    "            res_recall = recall(tp, fn)\n",
    "            res_f1 = f1(res_percision, res_recall)\n",
    "            print(\"\\tMille tÃ¤psus on \" + str(round(res_percision, 2)) + \", saagis on \"  + str(round(res_recall, 2)), end=\"\")\n",
    "            print(\", ning F1 skoor on \" + str(round(res_f1, 2)), end=\"\\n\\n\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TÃ¤is teksti meetodiga, |\n",
      "\tkus otsib 5 vÃµtmesÃµna:\n",
      "\n",
      "\tAndis TextRank lÃ¤henemine vÃµtmesÃµnadeks:\n",
      "\telurikkus, linn, aasta, omavalitsus, inimene\n",
      "\tMille tÃ¤psus on 0.8, saagis on 0.8, ning F1 skoor on 0.8\n",
      "\n",
      "\tAndis LDA lÃ¤henemine vÃµtmesÃµnadeks:\n",
      "\telurikkus, linn, aasta, omavalitsus, inimene\n",
      "\tMille tÃ¤psus on 0.8, saagis on 0.8, ning F1 skoor on 0.8\n",
      "\n",
      "\tAndis LSA lÃ¤henemine vÃµtmesÃµnadeks:\n",
      "\telurikkus, linn, aasta, omavalitsus, inimene\n",
      "\tMille tÃ¤psus on 0.8, saagis on 0.8, ning F1 skoor on 0.8\n",
      "\n",
      "\tkus otsib 10 vÃµtmesÃµna:\n",
      "\n",
      "\tAndis TextRank lÃ¤henemine vÃµtmesÃµnadeks:\n",
      "\telurikkus, linn, aasta, omavalitsus, inimene, liik, loodus, rohevÃµrgustik, vald, uuring\n",
      "\tMille tÃ¤psus on 0.6, saagis on 0.6, ning F1 skoor on 0.6\n",
      "\n",
      "\tAndis LDA lÃ¤henemine vÃµtmesÃµnadeks:\n",
      "\telurikkus, linn, aasta, omavalitsus, inimene, kohalik, sageli, liik, uuring, elama\n",
      "\tMille tÃ¤psus on 0.5, saagis on 0.5, ning F1 skoor on 0.5\n",
      "\n",
      "\tAndis LSA lÃ¤henemine vÃµtmesÃµnadeks:\n",
      "\telurikkus, linn, aasta, omavalitsus, inimene, kohalik, sageli, liik, elama, vald\n",
      "\tMille tÃ¤psus on 0.5, saagis on 0.5, ning F1 skoor on 0.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "arr = []\n",
    "with open(\"artikkel_voru_linna_lehest.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = Text(\"\\n\".join(f.readlines())).tag_analysis()\n",
    "    print(\"TÃ¤is teksti meetodiga, |\")\n",
    "    arr = method_generator(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5st kÃµige sagedasemad vÃµtmesÃµnad on: \n",
      "elurikkus, linn, aasta, omavalitsus, inimene\n",
      "\n",
      "10st kÃµige sagedasemad vÃµtmesÃµnad on: \n",
      "elurikkus, linn, aasta, omavalitsus, inimene, kohalik, loodus, sageli, liik, rohevÃµrgustik\n",
      "\n"
     ]
    }
   ],
   "source": [
    "positions = [Counter(), Counter()]\n",
    "counts = [Counter(), Counter()]\n",
    "\n",
    "short = [x for x in arr if len(x) == 5]\n",
    "long  = [x for x in arr if len(x) == 10]\n",
    "\n",
    "for i, sizes in enumerate([short, long]):\n",
    "    for case in sizes:\n",
    "        for pos, elem in enumerate(case):\n",
    "            positions[i][elem] += pos\n",
    "            counts[i][elem] += 1\n",
    "\n",
    "total = [Counter(), Counter()]\n",
    "            \n",
    "for i, reps in enumerate([5, 10]):\n",
    "    for item in counts[i]:\n",
    "        total[i][item] = reps - positions[i][item] / counts[i][item]\n",
    "\n",
    "for i, reps in enumerate([5, 10]):\n",
    "    print(str(reps) + \"st kÃµige sagedasemad vÃµtmesÃµnad on: \")\n",
    "    print(\", \".join([x[0] for x in total[i].most_common(reps)]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method 1</th>\n",
       "      <th>method 2</th>\n",
       "      <th>nr of keywords</th>\n",
       "      <th>overlap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kuldvÃµtmesÃµnade</td>\n",
       "      <td>TextRank</td>\n",
       "      <td>5</td>\n",
       "      <td>80.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kuldvÃµtmesÃµnade</td>\n",
       "      <td>TextRank</td>\n",
       "      <td>10</td>\n",
       "      <td>60.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kuldvÃµtmesÃµnade</td>\n",
       "      <td>LSA</td>\n",
       "      <td>5</td>\n",
       "      <td>80.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kuldvÃµtmesÃµnade</td>\n",
       "      <td>LSA</td>\n",
       "      <td>10</td>\n",
       "      <td>50.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kuldvÃµtmesÃµnade</td>\n",
       "      <td>LDA</td>\n",
       "      <td>5</td>\n",
       "      <td>80.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>kuldvÃµtmesÃµnade</td>\n",
       "      <td>LDA</td>\n",
       "      <td>10</td>\n",
       "      <td>50.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TextRank</td>\n",
       "      <td>LSA</td>\n",
       "      <td>5</td>\n",
       "      <td>100.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TextRank</td>\n",
       "      <td>LSA</td>\n",
       "      <td>10</td>\n",
       "      <td>70.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TextRank</td>\n",
       "      <td>LDA</td>\n",
       "      <td>5</td>\n",
       "      <td>100.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TextRank</td>\n",
       "      <td>LDA</td>\n",
       "      <td>10</td>\n",
       "      <td>70.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LSA</td>\n",
       "      <td>LDA</td>\n",
       "      <td>5</td>\n",
       "      <td>100.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LSA</td>\n",
       "      <td>LDA</td>\n",
       "      <td>10</td>\n",
       "      <td>90.0%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           method 1  method 2  nr of keywords overlap\n",
       "0   kuldvÃµtmesÃµnade  TextRank               5   80.0%\n",
       "1   kuldvÃµtmesÃµnade  TextRank              10   60.0%\n",
       "2   kuldvÃµtmesÃµnade       LSA               5   80.0%\n",
       "3   kuldvÃµtmesÃµnade       LSA              10   50.0%\n",
       "4   kuldvÃµtmesÃµnade       LDA               5   80.0%\n",
       "5   kuldvÃµtmesÃµnade       LDA              10   50.0%\n",
       "6          TextRank       LSA               5  100.0%\n",
       "7          TextRank       LSA              10   70.0%\n",
       "8          TextRank       LDA               5  100.0%\n",
       "9          TextRank       LDA              10   70.0%\n",
       "10              LSA       LDA               5  100.0%\n",
       "11              LSA       LDA              10   90.0%"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "real_keywords = [real_5_keywords, real_10_keywords]\n",
    "text_rank_keywords = [short[0], long[0]]\n",
    "lda_keywords = [short[1], long[1]]\n",
    "lsa_keywords = [short[2], long[2]]\n",
    "\n",
    "combinations = [(\"kuldvÃµtmesÃµnade\", real_keywords), (\"TextRank\", text_rank_keywords), \n",
    "               (\"LSA\", lsa_keywords), (\"LDA\", lda_keywords)]\n",
    "\n",
    "data = []\n",
    "for i in range(len(combinations) - 1):\n",
    "    for j in range(i + 1, len(combinations)):\n",
    "        for k in range(2):\n",
    "            first_name, first = combinations[i]\n",
    "            first_xs = first[k]\n",
    "            second_name, second = combinations[j]\n",
    "            second_xs = second[k]\n",
    "            overlap = sum([x in second_xs for x in first_xs]) / len(first_xs)\n",
    "            data.append([first_name, second_name, len(first_xs), str(overlap * 100) + \"%\"])\n",
    "\n",
    "pd.DataFrame(data, columns=[\"method 1\", \"method 2\", \"nr of keywords\", \"overlap\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hinnang:\n",
    "\n",
    "---\n",
    "\n",
    "LÃµppkokkuvÃµteks\n",
    " - kÃµik 3 erinevat vÃµtmesÃµnade eraldamise meetodit andsid vÃ¤ga sarnaseid tulemusi aga TextRank andis kÃµige paremaid tulemusi\n",
    " - KokkuvÃµtvate algoritmide F1 indeksid ei olnud kÃµige paremad. Aga suuresti vÃµib olla see tingitud sellest, et minu poolt valitud vÃµtmesÃµnad pole just kÃµige paremad esindamaks antud teksti\n",
    " - \"elurikkus\", \"aasta\", \"linn\", \"inimene\" ning \"omavalitsus\" on lÃ¤bivalt top 5 hulgas. Need 5 on kindlalt head vÃµtmesÃµna\n",
    " - LSA ning LDA annavad vÃ¤ga sarnaseid tulemusid kuid erinevaid kuldvÃµtmesÃµnadest ning TextRank vÃµtmesÃµnadest\n",
    " - Selle nÃ¤dala tulemused on kindlasti palju paremad kui eelmise nÃ¤dala omad. Ehk siis LDA ning LSA on head vahendid vÃµtmesÃµnade eraldamiseks.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMX96Ps7lrwPLT+kswPV0LP",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Quantitative measures of text complexity for the Independence Day speeches of the Presidents of the Republic of Estonia.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
