{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SYUrHxioHQxA"
   },
   "source": [
    "!pip install conllu estnltk==1.4.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "class TextRank4Keyword():\n",
    "    \"\"\"Extract keywords from text\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.d = 0.85 # damping coefficient, usually is .85\n",
    "        self.min_diff = 1e-5 # convergence threshold\n",
    "        self.steps = 10 # iteration steps\n",
    "        self.node_weight = None # save keywords and its weight\n",
    "\n",
    "    \n",
    "    def set_stopwords(self, stopwords):  \n",
    "        \"\"\"Set stop words\"\"\"\n",
    "        for word in STOP_WORDS.union(set(stopwords)):\n",
    "            lexeme = nlp.vocab[word]\n",
    "            lexeme.is_stop = True\n",
    "    \n",
    "    def sentence_segment(self, doc, candidate_pos, lower):\n",
    "        \"\"\"Store those words only in cadidate_pos\"\"\"\n",
    "        sentences = []\n",
    "        for sent in doc.sents:\n",
    "            selected_words = []\n",
    "            for token in sent:\n",
    "                # Store words only with cadidate POS tag\n",
    "                if token.pos_ in candidate_pos and token.is_stop is False:\n",
    "                    if lower is True:\n",
    "                        selected_words.append(token.text.lower())\n",
    "                    else:\n",
    "                        selected_words.append(token.text)\n",
    "            sentences.append(selected_words)\n",
    "        return sentences\n",
    "        \n",
    "    def get_vocab(self, sentences):\n",
    "        \"\"\"Get all tokens\"\"\"\n",
    "        vocab = OrderedDict()\n",
    "        i = 0\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = i\n",
    "                    i += 1\n",
    "        return vocab\n",
    "    \n",
    "    def get_token_pairs(self, window_size, sentences):\n",
    "        \"\"\"Build token_pairs from windows in sentences\"\"\"\n",
    "        token_pairs = list()\n",
    "        for sentence in sentences:\n",
    "            for i, word in enumerate(sentence):\n",
    "                for j in range(i+1, i+window_size):\n",
    "                    if j >= len(sentence):\n",
    "                        break\n",
    "                    pair = (word, sentence[j])\n",
    "                    if pair not in token_pairs:\n",
    "                        token_pairs.append(pair)\n",
    "        return token_pairs\n",
    "        \n",
    "    def symmetrize(self, a):\n",
    "        return a + a.T - np.diag(a.diagonal())\n",
    "    \n",
    "    def get_matrix(self, vocab, token_pairs):\n",
    "        \"\"\"Get normalized matrix\"\"\"\n",
    "        # Build matrix\n",
    "        vocab_size = len(vocab)\n",
    "        g = np.zeros((vocab_size, vocab_size), dtype='float')\n",
    "        for word1, word2 in token_pairs:\n",
    "            i, j = vocab[word1], vocab[word2]\n",
    "            g[i][j] = 1\n",
    "            \n",
    "        # Get Symmeric matrix\n",
    "        g = self.symmetrize(g)\n",
    "        \n",
    "        # Normalize matrix by column\n",
    "        norm = np.sum(g, axis=0)\n",
    "        g_norm = np.divide(g, norm, where=norm!=0) # this is ignore the 0 element in norm\n",
    "        \n",
    "        return g_norm\n",
    "\n",
    "    \n",
    "    def get_keywords(self, number=10):\n",
    "        \"\"\"Print top number keywords\"\"\"\n",
    "        node_weight = OrderedDict(sorted(self.node_weight.items(), key=lambda t: t[1], reverse=True))\n",
    "        return node_weight.items()\n",
    "    \n",
    "        \n",
    "    def analyze(self, text, \n",
    "                candidate_pos=['NOUN', 'PROPN'], \n",
    "                window_size=4, lower=False, stopwords=list()):\n",
    "        \"\"\"Main function to analyze text\"\"\"\n",
    "        \n",
    "        # Set stop words\n",
    "        self.set_stopwords(stopwords)\n",
    "        \n",
    "        # Pare text by spaCy\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        # Filter sentences\n",
    "        sentences = self.sentence_segment(doc, candidate_pos, lower) # list of list of words\n",
    "        \n",
    "        # Build vocabulary\n",
    "        vocab = self.get_vocab(sentences)\n",
    "        \n",
    "        # Get token_pairs from windows\n",
    "        token_pairs = self.get_token_pairs(window_size, sentences)\n",
    "        \n",
    "        # Get normalized matrix\n",
    "        g = self.get_matrix(vocab, token_pairs)\n",
    "        \n",
    "        # Initionlization for weight(pagerank value)\n",
    "        pr = np.array([1] * len(vocab))\n",
    "        \n",
    "        # Iteration\n",
    "        previous_pr = 0\n",
    "        for epoch in range(self.steps):\n",
    "            pr = (1-self.d) + self.d * np.dot(g, pr)\n",
    "            if abs(previous_pr - sum(pr))  < self.min_diff:\n",
    "                break\n",
    "            else:\n",
    "                previous_pr = sum(pr)\n",
    "\n",
    "        # Get weight for each node\n",
    "        node_weight = dict()\n",
    "        for word, index in vocab.items():\n",
    "            node_weight[word] = pr[index]\n",
    "        \n",
    "        self.node_weight = node_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors # to load the model\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format('lemmas.sg.s200.w2v.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(tp, fp):\n",
    "    return tp / (tp + fp)\n",
    "\n",
    "def recall(tp, fn):\n",
    "    return tp / (tp + fn)\n",
    "\n",
    "def f1(p, r):\n",
    "    return 2 * (p * r) / (p + r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_5_keywords = set([\"elurikkus\", \"aasta\", \"omavalitsus\", \"inimene\", \"rohevõrgustik\"])\n",
    "real_10_keywords = real_5_keywords | set([\"hoidmine\", \"liik\", \"keskkond\", \"kultuur\", \"planeering\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dxENjZ3yMbBN",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from estnltk import Text\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "import html\n",
    "from collections import Counter\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy\n",
    "import networkx as nx\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "\n",
    "\n",
    "def is_text(word):\n",
    "    return len(set(word[\"analysis\"][0][\"partofspeech\"]) & set(['Z'])) < 1\n",
    "\n",
    "def is_substantive(word):\n",
    "    return len(set(word[\"analysis\"][0][\"partofspeech\"]) & set(['S'])) > 0\n",
    "\n",
    "def is_adjective_or_substantive(word):\n",
    "    return len(set(word[\"analysis\"][0][\"partofspeech\"]) & set(['S', 'A'])) > 0\n",
    "\n",
    "stop_words = [x.strip() for x in open(\"estonian-stopwords-lemmas.txt\", \"r\", encoding=\"utf-8\").readlines()]\n",
    "set_stop_words = set(stop_words)\n",
    "\n",
    "def method_generator(text):\n",
    "    textRank = Counter()\n",
    "    \n",
    "    lemma_text = \"\"\n",
    "    un_stopped_text = []\n",
    "    \n",
    "    for word in text.words:\n",
    "        lemma = word[\"analysis\"][0][\"lemma\"]\n",
    "        lemma_text += lemma + \" \"\n",
    "        if lemma not in stop_words and is_text(word):\n",
    "            un_stopped_text.append(lemma)\n",
    "        \n",
    "        if is_substantive(word):\n",
    "            textRank[lemma] += 1\n",
    "    \n",
    "    tr4w = TextRank4Keyword()\n",
    "    tr4w.analyze(lemma_text, candidate_pos = ['NOUN'], window_size=4, lower=False, stopwords=stop_words)\n",
    "    \n",
    "    LDA_texts = [un_stopped_text]\n",
    "    dictionary = corpora.Dictionary(LDA_texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in LDA_texts]\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=1, id2word = dictionary, passes=20)\n",
    "    \n",
    "    LSA_texts = [un_stopped_text]\n",
    "    dictionary = corpora.Dictionary(LSA_texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in LSA_texts]\n",
    "    lsamodel = gensim.models.LsiModel(corpus, num_topics=1, id2word = dictionary)\n",
    "    \n",
    "    def get_text_rank_words_from_model(model, nr_of_words):\n",
    "        method_res = method.most_common(nr_of_words)\n",
    "        return [x for x, y in method_res]\n",
    "    \n",
    "    def get_words_from_model(model, nr_of_words):\n",
    "        return [x.split('\"')[1] for x in model.print_topics(num_words=nr_of_words)[0][1].split(\"+\")]\n",
    "\n",
    "    sets = [(5, real_5_keywords), (10, real_10_keywords)]\n",
    "    methods = [(\"TextRank\", textRank, get_text_rank_words_from_model),\n",
    "               (\"LDA\", ldamodel, get_words_from_model),\n",
    "               (\"LSA\", lsamodel, get_words_from_model)]\n",
    "    results = []\n",
    "    for count, answer in sets:\n",
    "\n",
    "        print(\"\\tkus otsib \" + str(count) + \" võtmesõna:\\n\")\n",
    "        for prefix, method, resolver in methods:\n",
    "            res = resolver(method, count)\n",
    "            results.append(res)\n",
    "            print(\"\\tAndis \" + prefix + \" lähenemine võtmesõnadeks:\\n\\t\" + \", \".join(res))\n",
    "            tp = len(answer & set(res))\n",
    "            fp = len(set(res) - answer)\n",
    "            fn = len(answer - set(res))\n",
    "            res_percision = precision(tp, fp)\n",
    "            res_recall = recall(tp, fn)\n",
    "            res_f1 = f1(res_percision, res_recall)\n",
    "            print(\"\\tMille täpsus on \" + str(round(res_percision, 2)) + \", saagis on \"  + str(round(res_recall, 2)), end=\"\")\n",
    "            print(\", ning F1 skoor on \" + str(round(res_f1, 2)), end=\"\\n\\n\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Täis teksti meetodiga, |\n",
      "\tkus otsib 5 võtmesõna:\n",
      "\n",
      "\tAndis TextRank lähenemine võtmesõnadeks:\n",
      "\telurikkus, linn, aasta, omavalitsus, inimene\n",
      "\tMille täpsus on 0.8, saagis on 0.8, ning F1 skoor on 0.8\n",
      "\n",
      "\tAndis LDA lähenemine võtmesõnadeks:\n",
      "\telurikkus, linn, aasta, omavalitsus, inimene\n",
      "\tMille täpsus on 0.8, saagis on 0.8, ning F1 skoor on 0.8\n",
      "\n",
      "\tAndis LSA lähenemine võtmesõnadeks:\n",
      "\telurikkus, linn, aasta, omavalitsus, inimene\n",
      "\tMille täpsus on 0.8, saagis on 0.8, ning F1 skoor on 0.8\n",
      "\n",
      "\tkus otsib 10 võtmesõna:\n",
      "\n",
      "\tAndis TextRank lähenemine võtmesõnadeks:\n",
      "\telurikkus, linn, aasta, omavalitsus, inimene, liik, loodus, rohevõrgustik, vald, uuring\n",
      "\tMille täpsus on 0.6, saagis on 0.6, ning F1 skoor on 0.6\n",
      "\n",
      "\tAndis LDA lähenemine võtmesõnadeks:\n",
      "\telurikkus, linn, aasta, omavalitsus, inimene, kohalik, sageli, liik, uuring, elama\n",
      "\tMille täpsus on 0.5, saagis on 0.5, ning F1 skoor on 0.5\n",
      "\n",
      "\tAndis LSA lähenemine võtmesõnadeks:\n",
      "\telurikkus, linn, aasta, omavalitsus, inimene, kohalik, sageli, liik, elama, vald\n",
      "\tMille täpsus on 0.5, saagis on 0.5, ning F1 skoor on 0.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "arr = []\n",
    "with open(\"artikkel_voru_linna_lehest.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = Text(\"\\n\".join(f.readlines())).tag_analysis()\n",
    "    print(\"Täis teksti meetodiga, |\")\n",
    "    arr = method_generator(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5st kõige sagedasemad võtmesõnad on: \n",
      "elurikkus, linn, aasta, omavalitsus, inimene\n",
      "\n",
      "10st kõige sagedasemad võtmesõnad on: \n",
      "elurikkus, linn, aasta, omavalitsus, inimene, kohalik, loodus, sageli, liik, rohevõrgustik\n",
      "\n"
     ]
    }
   ],
   "source": [
    "positions = [Counter(), Counter()]\n",
    "counts = [Counter(), Counter()]\n",
    "\n",
    "short = [x for x in arr if len(x) == 5]\n",
    "long  = [x for x in arr if len(x) == 10]\n",
    "\n",
    "for i, sizes in enumerate([short, long]):\n",
    "    for case in sizes:\n",
    "        for pos, elem in enumerate(case):\n",
    "            positions[i][elem] += pos\n",
    "            counts[i][elem] += 1\n",
    "\n",
    "total = [Counter(), Counter()]\n",
    "            \n",
    "for i, reps in enumerate([5, 10]):\n",
    "    for item in counts[i]:\n",
    "        total[i][item] = reps - positions[i][item] / counts[i][item]\n",
    "\n",
    "for i, reps in enumerate([5, 10]):\n",
    "    print(str(reps) + \"st kõige sagedasemad võtmesõnad on: \")\n",
    "    print(\", \".join([x[0] for x in total[i].most_common(reps)]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method 1</th>\n",
       "      <th>method 2</th>\n",
       "      <th>nr of keywords</th>\n",
       "      <th>overlap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kuldvõtmesõnade</td>\n",
       "      <td>TextRank</td>\n",
       "      <td>5</td>\n",
       "      <td>80.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kuldvõtmesõnade</td>\n",
       "      <td>TextRank</td>\n",
       "      <td>10</td>\n",
       "      <td>60.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kuldvõtmesõnade</td>\n",
       "      <td>LSA</td>\n",
       "      <td>5</td>\n",
       "      <td>80.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kuldvõtmesõnade</td>\n",
       "      <td>LSA</td>\n",
       "      <td>10</td>\n",
       "      <td>50.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kuldvõtmesõnade</td>\n",
       "      <td>LDA</td>\n",
       "      <td>5</td>\n",
       "      <td>80.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>kuldvõtmesõnade</td>\n",
       "      <td>LDA</td>\n",
       "      <td>10</td>\n",
       "      <td>50.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TextRank</td>\n",
       "      <td>LSA</td>\n",
       "      <td>5</td>\n",
       "      <td>100.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TextRank</td>\n",
       "      <td>LSA</td>\n",
       "      <td>10</td>\n",
       "      <td>70.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TextRank</td>\n",
       "      <td>LDA</td>\n",
       "      <td>5</td>\n",
       "      <td>100.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TextRank</td>\n",
       "      <td>LDA</td>\n",
       "      <td>10</td>\n",
       "      <td>70.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LSA</td>\n",
       "      <td>LDA</td>\n",
       "      <td>5</td>\n",
       "      <td>100.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LSA</td>\n",
       "      <td>LDA</td>\n",
       "      <td>10</td>\n",
       "      <td>90.0%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           method 1  method 2  nr of keywords overlap\n",
       "0   kuldvõtmesõnade  TextRank               5   80.0%\n",
       "1   kuldvõtmesõnade  TextRank              10   60.0%\n",
       "2   kuldvõtmesõnade       LSA               5   80.0%\n",
       "3   kuldvõtmesõnade       LSA              10   50.0%\n",
       "4   kuldvõtmesõnade       LDA               5   80.0%\n",
       "5   kuldvõtmesõnade       LDA              10   50.0%\n",
       "6          TextRank       LSA               5  100.0%\n",
       "7          TextRank       LSA              10   70.0%\n",
       "8          TextRank       LDA               5  100.0%\n",
       "9          TextRank       LDA              10   70.0%\n",
       "10              LSA       LDA               5  100.0%\n",
       "11              LSA       LDA              10   90.0%"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "real_keywords = [real_5_keywords, real_10_keywords]\n",
    "text_rank_keywords = [short[0], long[0]]\n",
    "lda_keywords = [short[1], long[1]]\n",
    "lsa_keywords = [short[2], long[2]]\n",
    "\n",
    "combinations = [(\"kuldvõtmesõnade\", real_keywords), (\"TextRank\", text_rank_keywords), \n",
    "               (\"LSA\", lsa_keywords), (\"LDA\", lda_keywords)]\n",
    "\n",
    "data = []\n",
    "for i in range(len(combinations) - 1):\n",
    "    for j in range(i + 1, len(combinations)):\n",
    "        for k in range(2):\n",
    "            first_name, first = combinations[i]\n",
    "            first_xs = first[k]\n",
    "            second_name, second = combinations[j]\n",
    "            second_xs = second[k]\n",
    "            overlap = sum([x in second_xs for x in first_xs]) / len(first_xs)\n",
    "            data.append([first_name, second_name, len(first_xs), str(overlap * 100) + \"%\"])\n",
    "\n",
    "pd.DataFrame(data, columns=[\"method 1\", \"method 2\", \"nr of keywords\", \"overlap\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hinnang:\n",
    "\n",
    "---\n",
    "\n",
    "Lõppkokkuvõteks\n",
    " - kõik 3 erinevat võtmesõnade eraldamise meetodit andsid väga sarnaseid tulemusi aga TextRank andis kõige paremaid tulemusi\n",
    " - Kokkuvõtvate algoritmide F1 indeksid ei olnud kõige paremad. Aga suuresti võib olla see tingitud sellest, et minu poolt valitud võtmesõnad pole just kõige paremad esindamaks antud teksti\n",
    " - \"elurikkus\", \"aasta\", \"linn\", \"inimene\" ning \"omavalitsus\" on läbivalt top 5 hulgas. Need 5 on kindlalt head võtmesõna\n",
    " - LSA ning LDA annavad väga sarnaseid tulemusid kuid erinevaid kuldvõtmesõnadest ning TextRank võtmesõnadest\n",
    " - Selle nädala tulemused on kindlasti palju paremad kui eelmise nädala omad. Ehk siis LDA ning LSA on head vahendid võtmesõnade eraldamiseks.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMX96Ps7lrwPLT+kswPV0LP",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Quantitative measures of text complexity for the Independence Day speeches of the Presidents of the Republic of Estonia.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
