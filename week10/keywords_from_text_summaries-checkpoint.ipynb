{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SYUrHxioHQxA"
   },
   "source": [
    "!pip install conllu estnltk==1.4.1.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "class TextRank4Keyword():\n",
    "    \"\"\"Extract keywords from text\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.d = 0.85 # damping coefficient, usually is .85\n",
    "        self.min_diff = 1e-5 # convergence threshold\n",
    "        self.steps = 10 # iteration steps\n",
    "        self.node_weight = None # save keywords and its weight\n",
    "\n",
    "    \n",
    "    def set_stopwords(self, stopwords):  \n",
    "        \"\"\"Set stop words\"\"\"\n",
    "        for word in STOP_WORDS.union(set(stopwords)):\n",
    "            lexeme = nlp.vocab[word]\n",
    "            lexeme.is_stop = True\n",
    "    \n",
    "    def sentence_segment(self, doc, candidate_pos, lower):\n",
    "        \"\"\"Store those words only in cadidate_pos\"\"\"\n",
    "        sentences = []\n",
    "        for sent in doc.sents:\n",
    "            selected_words = []\n",
    "            for token in sent:\n",
    "                # Store words only with cadidate POS tag\n",
    "                if token.pos_ in candidate_pos and token.is_stop is False:\n",
    "                    if lower is True:\n",
    "                        selected_words.append(token.text.lower())\n",
    "                    else:\n",
    "                        selected_words.append(token.text)\n",
    "            sentences.append(selected_words)\n",
    "        return sentences\n",
    "        \n",
    "    def get_vocab(self, sentences):\n",
    "        \"\"\"Get all tokens\"\"\"\n",
    "        vocab = OrderedDict()\n",
    "        i = 0\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = i\n",
    "                    i += 1\n",
    "        return vocab\n",
    "    \n",
    "    def get_token_pairs(self, window_size, sentences):\n",
    "        \"\"\"Build token_pairs from windows in sentences\"\"\"\n",
    "        token_pairs = list()\n",
    "        for sentence in sentences:\n",
    "            for i, word in enumerate(sentence):\n",
    "                for j in range(i+1, i+window_size):\n",
    "                    if j >= len(sentence):\n",
    "                        break\n",
    "                    pair = (word, sentence[j])\n",
    "                    if pair not in token_pairs:\n",
    "                        token_pairs.append(pair)\n",
    "        return token_pairs\n",
    "        \n",
    "    def symmetrize(self, a):\n",
    "        return a + a.T - np.diag(a.diagonal())\n",
    "    \n",
    "    def get_matrix(self, vocab, token_pairs):\n",
    "        \"\"\"Get normalized matrix\"\"\"\n",
    "        # Build matrix\n",
    "        vocab_size = len(vocab)\n",
    "        g = np.zeros((vocab_size, vocab_size), dtype='float')\n",
    "        for word1, word2 in token_pairs:\n",
    "            i, j = vocab[word1], vocab[word2]\n",
    "            g[i][j] = 1\n",
    "            \n",
    "        # Get Symmeric matrix\n",
    "        g = self.symmetrize(g)\n",
    "        \n",
    "        # Normalize matrix by column\n",
    "        norm = np.sum(g, axis=0)\n",
    "        g_norm = np.divide(g, norm, where=norm!=0) # this is ignore the 0 element in norm\n",
    "        \n",
    "        return g_norm\n",
    "\n",
    "    \n",
    "    def get_keywords(self, number=10):\n",
    "        \"\"\"Print top number keywords\"\"\"\n",
    "        node_weight = OrderedDict(sorted(self.node_weight.items(), key=lambda t: t[1], reverse=True))\n",
    "        return node_weight.items()\n",
    "    \n",
    "        \n",
    "    def analyze(self, text, \n",
    "                candidate_pos=['NOUN', 'PROPN'], \n",
    "                window_size=4, lower=False, stopwords=list()):\n",
    "        \"\"\"Main function to analyze text\"\"\"\n",
    "        \n",
    "        # Set stop words\n",
    "        self.set_stopwords(stopwords)\n",
    "        \n",
    "        # Pare text by spaCy\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        # Filter sentences\n",
    "        sentences = self.sentence_segment(doc, candidate_pos, lower) # list of list of words\n",
    "        \n",
    "        # Build vocabulary\n",
    "        vocab = self.get_vocab(sentences)\n",
    "        \n",
    "        # Get token_pairs from windows\n",
    "        token_pairs = self.get_token_pairs(window_size, sentences)\n",
    "        \n",
    "        # Get normalized matrix\n",
    "        g = self.get_matrix(vocab, token_pairs)\n",
    "        \n",
    "        # Initionlization for weight(pagerank value)\n",
    "        pr = np.array([1] * len(vocab))\n",
    "        \n",
    "        # Iteration\n",
    "        previous_pr = 0\n",
    "        for epoch in range(self.steps):\n",
    "            pr = (1-self.d) + self.d * np.dot(g, pr)\n",
    "            if abs(previous_pr - sum(pr))  < self.min_diff:\n",
    "                break\n",
    "            else:\n",
    "                previous_pr = sum(pr)\n",
    "\n",
    "        # Get weight for each node\n",
    "        node_weight = dict()\n",
    "        for word, index in vocab.items():\n",
    "            node_weight[word] = pr[index]\n",
    "        \n",
    "        self.node_weight = node_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors # to load the model\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format('lemmas.sg.s200.w2v.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(tp, fp):\n",
    "    return tp / (tp + fp)\n",
    "\n",
    "def recall(tp, fn):\n",
    "    return tp / (tp + fn)\n",
    "\n",
    "def f1(p, r):\n",
    "    return 2 * (p * r) / (p + r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_5_keywords = set([\"elurikkus\", \"aasta\", \"omavalitsus\", \"inimene\", \"rohevõrgustik\"])\n",
    "real_10_keywords = real_5_keywords | set([\"hoidmine\", \"liik\", \"keskkond\", \"kultuur\", \"planeering\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dxENjZ3yMbBN",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Täis teksti meetodiga, |\n",
      "\tkus otsib 5 võtmesõna:\n",
      "\n",
      "\tAndis Esimese lähenemine võtmesõnadeks:\n",
      "\telurikkus, linn, aasta, omavalitsus, inimene\n",
      "\tMille täpsus on 0.8, saagis on 0.8, ning F1 skoor on 0.8\n",
      "\n",
      "\tAndis Teise lähenemine võtmesõnadeks:\n",
      "\telurikkus, aasta, omavalitsus, inimene, liik\n",
      "\tMille täpsus on 0.8, saagis on 0.8, ning F1 skoor on 0.8\n",
      "\n",
      "\tAndis Kolmanda lähenemine võtmesõnadeks:\n",
      "\telurikkus, aasta, omavalitsus, koht, kord\n",
      "\tMille täpsus on 0.6, saagis on 0.6, ning F1 skoor on 0.6\n",
      "\n",
      "\tkus otsib 10 võtmesõna:\n",
      "\n",
      "\tAndis Esimese lähenemine võtmesõnadeks:\n",
      "\telurikkus, linn, aasta, omavalitsus, inimene, kohalik, liik, uuring, rohevõrgustik, vald\n",
      "\tMille täpsus on 0.6, saagis on 0.6, ning F1 skoor on 0.6\n",
      "\n",
      "\tAndis Teise lähenemine võtmesõnadeks:\n",
      "\telurikkus, aasta, omavalitsus, inimene, liik, bioloog, koht, rohevõrgustik, lind, maa\n",
      "\tMille täpsus on 0.6, saagis on 0.6, ning F1 skoor on 0.6\n",
      "\n",
      "\tAndis Kolmanda lähenemine võtmesõnadeks:\n",
      "\telurikkus, aasta, omavalitsus, koht, kord, väärtus, teadvus, võimalus, valik, loeng\n",
      "\tMille täpsus on 0.3, saagis on 0.3, ning F1 skoor on 0.3\n",
      "\n",
      "Esimese tekstikokkuvõtmise algoritmiga, kus valiti 5 lauset, |\n",
      "\tkus otsib 5 võtmesõna:\n",
      "\n",
      "\tAndis Esimese lähenemine võtmesõnadeks:\n",
      "\telurikkus, linn, omavalitsus, vald, planeerimine\n",
      "\tMille täpsus on 0.4, saagis on 0.4, ning F1 skoor on 0.4\n",
      "\n",
      "\tAndis Teise lähenemine võtmesõnadeks:\n",
      "\telurikkus, omavalitsus, sissetoomine, planeerimispraktika, hoidmine\n",
      "\tMille täpsus on 0.4, saagis on 0.4, ning F1 skoor on 0.4\n",
      "\n",
      "\tAndis Kolmanda lähenemine võtmesõnadeks:\n",
      "\telurikkus, omavalitsus, sissetoomine, planeerimispraktika, hoidmine\n",
      "\tMille täpsus on 0.4, saagis on 0.4, ning F1 skoor on 0.4\n",
      "\n",
      "\tkus otsib 10 võtmesõna:\n",
      "\n",
      "\tAndis Esimese lähenemine võtmesõnadeks:\n",
      "\telurikkus, linn, omavalitsus, vald, planeerimine, oluline, kohalik, tehtud, olemasolev, suunanäitaja\n",
      "\tMille täpsus on 0.2, saagis on 0.2, ning F1 skoor on 0.2\n",
      "\n",
      "\tAndis Teise lähenemine võtmesõnadeks:\n",
      "\telurikkus, omavalitsus, sissetoomine, planeerimispraktika, hoidmine, kaitsmine, toetav, saatus, toetaja, väärtus\n",
      "\tMille täpsus on 0.3, saagis on 0.3, ning F1 skoor on 0.3\n",
      "\n",
      "\tAndis Kolmanda lähenemine võtmesõnadeks:\n",
      "\telurikkus, omavalitsus, sissetoomine, planeerimispraktika, hoidmine, saatus, toetaja, väärtus, kaitsja, mäng\n",
      "\tMille täpsus on 0.3, saagis on 0.3, ning F1 skoor on 0.3\n",
      "\n",
      "Esimese tekstikokkuvõtmise algoritmiga, kus valiti 10 lauset, |\n",
      "\tkus otsib 5 võtmesõna:\n",
      "\n",
      "\tAndis Esimese lähenemine võtmesõnadeks:\n",
      "\telurikkus, linn, omavalitsus, toetav, oluline\n",
      "\tMille täpsus on 0.4, saagis on 0.4, ning F1 skoor on 0.4\n",
      "\n",
      "\tAndis Teise lähenemine võtmesõnadeks:\n",
      "\telurikkus, omavalitsus, toetav, väärtus, hoidmine\n",
      "\tMille täpsus on 0.4, saagis on 0.4, ning F1 skoor on 0.4\n",
      "\n",
      "\tAndis Kolmanda lähenemine võtmesõnadeks:\n",
      "\telurikkus, omavalitsus, väärtus, hoidmine, vihmapeenar\n",
      "\tMille täpsus on 0.4, saagis on 0.4, ning F1 skoor on 0.4\n",
      "\n",
      "\tkus otsib 10 võtmesõna:\n",
      "\n",
      "\tAndis Esimese lähenemine võtmesõnadeks:\n",
      "\telurikkus, linn, omavalitsus, toetav, oluline, vald, väärtus, planeerimine, keskkonnamõju, aasta\n",
      "\tMille täpsus on 0.3, saagis on 0.3, ning F1 skoor on 0.3\n",
      "\n",
      "\tAndis Teise lähenemine võtmesõnadeks:\n",
      "\telurikkus, omavalitsus, toetav, väärtus, hoidmine, immutamine, ala, vihmapeenar, sissetoomine, planeerimispraktika\n",
      "\tMille täpsus on 0.3, saagis on 0.3, ning F1 skoor on 0.3\n",
      "\n",
      "\tAndis Kolmanda lähenemine võtmesõnadeks:\n",
      "\telurikkus, omavalitsus, väärtus, hoidmine, vihmapeenar, immutamine, ala, sissetoomine, planeerimispraktika, staadium\n",
      "\tMille täpsus on 0.3, saagis on 0.3, ning F1 skoor on 0.3\n",
      "\n",
      "Teise tekstikokkuvõtmise algoritmiga, kus valiti 5 lauset, |\n",
      "\tkus otsib 5 võtmesõna:\n",
      "\n",
      "\tAndis Esimese lähenemine võtmesõnadeks:\n",
      "\taasta, inimene, taim, ala, elurikas\n",
      "\tMille täpsus on 0.4, saagis on 0.4, ning F1 skoor on 0.4\n",
      "\n",
      "\tAndis Teise lähenemine võtmesõnadeks:\n",
      "\tinimene, aasta, taim, artikkel, leht\n",
      "\tMille täpsus on 0.4, saagis on 0.4, ning F1 skoor on 0.4\n",
      "\n",
      "\tAndis Kolmanda lähenemine võtmesõnadeks:\n",
      "\tinimene, aasta, taim, artikkel, leht\n",
      "\tMille täpsus on 0.4, saagis on 0.4, ning F1 skoor on 0.4\n",
      "\n",
      "\tkus otsib 10 võtmesõna:\n",
      "\n",
      "\tAndis Esimese lähenemine võtmesõnadeks:\n",
      "\taasta, inimene, taim, ala, elurikas, linn, artikkel, pargitee, nurm, monokultuurne\n",
      "\tMille täpsus on 0.2, saagis on 0.2, ning F1 skoor on 0.2\n",
      "\n",
      "\tAndis Teise lähenemine võtmesõnadeks:\n",
      "\tinimene, aasta, taim, artikkel, leht, meetod, toimetulek, senine, steriilne, teadvus\n",
      "\tMille täpsus on 0.2, saagis on 0.2, ning F1 skoor on 0.2\n",
      "\n",
      "\tAndis Kolmanda lähenemine võtmesõnadeks:\n",
      "\tinimene, aasta, taim, artikkel, leht, meetod, toimetulek, teadvus, lood, haljasala\n",
      "\tMille täpsus on 0.2, saagis on 0.2, ning F1 skoor on 0.2\n",
      "\n",
      "Teise tekstikokkuvõtmise algoritmiga, kus valiti 10 lauset, |\n",
      "\tkus otsib 5 võtmesõna:\n",
      "\n",
      "\tAndis Esimese lähenemine võtmesõnadeks:\n",
      "\taasta, uuring, inimene, linn, taim\n",
      "\tMille täpsus on 0.4, saagis on 0.4, ning F1 skoor on 0.4\n",
      "\n",
      "\tAndis Teise lähenemine võtmesõnadeks:\n",
      "\taasta, inimene, taim, sotsiaalne, artikkel\n",
      "\tMille täpsus on 0.4, saagis on 0.4, ning F1 skoor on 0.4\n",
      "\n",
      "\tAndis Kolmanda lähenemine võtmesõnadeks:\n",
      "\taasta, inimene, taim, artikkel, uuring\n",
      "\tMille täpsus on 0.4, saagis on 0.4, ning F1 skoor on 0.4\n",
      "\n",
      "\tkus otsib 10 võtmesõna:\n",
      "\n",
      "\tAndis Esimese lähenemine võtmesõnadeks:\n",
      "\taasta, uuring, inimene, linn, taim, ala, keskkond, vald, elurikkus, elurikas\n",
      "\tMille täpsus on 0.4, saagis on 0.4, ning F1 skoor on 0.4\n",
      "\n",
      "\tAndis Teise lähenemine võtmesõnadeks:\n",
      "\taasta, inimene, taim, sotsiaalne, artikkel, uuring, leht, teadlane, umbrohi, taimestik\n",
      "\tMille täpsus on 0.2, saagis on 0.2, ning F1 skoor on 0.2\n",
      "\n",
      "\tAndis Kolmanda lähenemine võtmesõnadeks:\n",
      "\taasta, inimene, taim, artikkel, uuring, leht, umbrohi, meetod, lisa, rohevõrgustik\n",
      "\tMille täpsus on 0.3, saagis on 0.3, ning F1 skoor on 0.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from estnltk import Text\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "import html\n",
    "from collections import Counter\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy\n",
    "import networkx as nx\n",
    "\n",
    "def is_substantive(word):\n",
    "    return len(set(word[\"analysis\"][0][\"partofspeech\"]) & set(['S'])) > 0\n",
    "\n",
    "def is_adjective_or_substantive(word):\n",
    "    return len(set(word[\"analysis\"][0][\"partofspeech\"]) & set(['S', 'A'])) > 0\n",
    "\n",
    "stop_words = [x.strip() for x in open(\"estonian-stopwords-lemmas.txt\", \"r\", encoding=\"utf-8\").readlines()]\n",
    "set_stop_words = set(stop_words)\n",
    "\n",
    "def method_generator(text):\n",
    "    method1 = Counter()\n",
    "    lemma_text = \"\"\n",
    "    \n",
    "    for word in text.words:\n",
    "        lemma = word[\"analysis\"][0][\"lemma\"]\n",
    "        lemma_text += lemma + \" \"\n",
    "        if is_adjective_or_substantive(word):\n",
    "            method1[lemma] += 1\n",
    "    tr4w = TextRank4Keyword()\n",
    "    tr4w.analyze(lemma_text, candidate_pos = ['NOUN', 'PROPN'], window_size=4, lower=False, stopwords=stop_words)\n",
    "    method2 = Counter()\n",
    "    for item, value in tr4w.get_keywords():\n",
    "        text_item = Text(item).tag_analysis()\n",
    "        if is_adjective_or_substantive(text_item.words[0]):\n",
    "            method2[item] = value\n",
    "    \n",
    "    method3 = []\n",
    "    groups = []\n",
    "\n",
    "    for item, value in method2.items():\n",
    "        text_item = Text(item).tag_analysis()\n",
    "        if is_substantive(text_item.words[0]):\n",
    "            method3.append(item)\n",
    "\n",
    "    for item in method3:\n",
    "        added = False\n",
    "        for group in groups:\n",
    "            if not added:\n",
    "                for other in group:\n",
    "                    try:\n",
    "                        if model.similarity(item, other) > 0.5:\n",
    "                            group.append(item)\n",
    "                            added = True\n",
    "                            break\n",
    "                    except Exception:\n",
    "                        pass\n",
    "\n",
    "        if not added:\n",
    "            groups.append([item])\n",
    "\n",
    "    method3 = Counter()\n",
    "    new_groups = []\n",
    "\n",
    "    for group in groups:\n",
    "        best = [0, \"\"]\n",
    "        for elem in group:\n",
    "            potential = method2[elem]\n",
    "            if potential > best[0]:\n",
    "                best = [method2[elem], elem]\n",
    "        method3[best[1]] = best[0]\n",
    "    \n",
    "    sets = [(5, real_5_keywords), (10, real_10_keywords)]\n",
    "    methods = [(\"Esimese\", method1), (\"Teise\", method2), (\"Kolmanda\", method3)]\n",
    "    results = []\n",
    "    for count, answer in sets:\n",
    "\n",
    "        print(\"\\tkus otsib \" + str(count) + \" võtmesõna:\\n\")\n",
    "        for prefix, method in methods:\n",
    "            method_res = method.most_common(count)\n",
    "            res = [x for x, y in method_res]\n",
    "            results.append(res)\n",
    "            print(\"\\tAndis \" + prefix + \" lähenemine võtmesõnadeks:\\n\\t\" + \", \".join(res))\n",
    "            tp = len(answer & set(res))\n",
    "            fp = len(set(res) - answer)\n",
    "            fn = len(answer - set(res))\n",
    "            res_percision = precision(tp, fp)\n",
    "            res_recall = recall(tp, fn)\n",
    "            res_f1 = f1(res_percision, res_recall)\n",
    "            print(\"\\tMille täpsus on \" + str(round(res_percision, 2)) + \", saagis on \"  + str(round(res_recall, 2)), end=\"\")\n",
    "            print(\", ning F1 skoor on \" + str(round(res_f1, 2)), end=\"\\n\\n\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def lemma_scores(text):\n",
    "    scores = Counter()\n",
    "    for word in text.words:\n",
    "        lemma = word[\"analysis\"][0][\"lemma\"]\n",
    "        if is_adjective_or_substantive(word) and lemma not in set_stop_words:\n",
    "            scores[lemma] += 1\n",
    "        else:\n",
    "            scores[lemma] = 0\n",
    "    return scores\n",
    "\n",
    "\n",
    "def first_text_aggregation_method(nr_of_sentences, text):\n",
    "    lemma_repetitions = lemma_scores(text)\n",
    "    sentence_scores = Counter()\n",
    "    for sentence in text.sentence_texts:\n",
    "        sentence = Text(sentence).tag_analysis()\n",
    "        score = 0\n",
    "        for word in sentence.words:\n",
    "            lemma = word[\"analysis\"][0][\"lemma\"]\n",
    "            score += lemma_repetitions[lemma]\n",
    "        score /= len(sentence.words)\n",
    "        sentence_scores[sentence.text] = score\n",
    "    text = \"\\n\".join([x[0] for x in sentence_scores.most_common(nr_of_sentences)])\n",
    "    return Text(text).tag_analysis()\n",
    "\n",
    "# def string_to_float(string):\n",
    "#     return float(\"\".join([str(ord(x)) for x in string]))\n",
    "\n",
    "def cosine_similarity_between_lemmas(this, other):\n",
    "#     this = [[string_to_float(x) for x in this]]\n",
    "#     other = [[string_to_float(x) for x in other]]\n",
    "#     return cosine_similarity(this, other)[0][0]\n",
    "    return sum([x in this for x in other])\n",
    "\n",
    "def second_text_aggregation_method(nr_of_sentences, text):\n",
    "    lemma_repetitions = lemma_scores(text)\n",
    "    sentences_dict = dict()\n",
    "    summarize_text = []\n",
    "    \n",
    "    for sentence in text.sentence_texts:\n",
    "        pre = sentence\n",
    "        sentence = Text(sentence).tag_analysis()\n",
    "        sentences_dict[pre] = [x[\"analysis\"][0][\"lemma\"] for x in sentence.words]\n",
    "\n",
    "    matrix = numpy.array([numpy.array([cosine_similarity_between_lemmas(this, other) for this in sentences_dict.values()])\n",
    "              for other in sentences_dict.values()])\n",
    "    sentence_similarity_graph = nx.from_numpy_array(matrix)\n",
    "    scores = nx.pagerank(sentence_similarity_graph)\n",
    "    \n",
    "    sentences = sentences_dict.keys()\n",
    "    ranked_sentences = sorted(((scores[i], sentence) for i, sentence in enumerate(sentences)), reverse=True) \n",
    "    \n",
    "    for i in range(nr_of_sentences):\n",
    "        summarize_text.append(ranked_sentences[i][1])\n",
    "    \n",
    "    text = \" \".join(summarize_text)\n",
    "    return Text(text).tag_analysis()\n",
    "\n",
    "\n",
    "arr = []\n",
    "with open(\"artikkel_voru_linna_lehest.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = Text(\"\\n\".join(f.readlines())).tag_analysis()\n",
    "    print(\"Täis teksti meetodiga, |\")\n",
    "    arr = method_generator(text)\n",
    "    print(\"Esimese tekstikokkuvõtmise algoritmiga, kus valiti 5 lauset, |\")\n",
    "    arr += method_generator(first_text_aggregation_method(5, text))\n",
    "    print(\"Esimese tekstikokkuvõtmise algoritmiga, kus valiti 10 lauset, |\")\n",
    "    arr += method_generator(first_text_aggregation_method(10, text))\n",
    "    print(\"Teise tekstikokkuvõtmise algoritmiga, kus valiti 5 lauset, |\")\n",
    "    arr += method_generator(second_text_aggregation_method(5, text))\n",
    "    print(\"Teise tekstikokkuvõtmise algoritmiga, kus valiti 10 lauset, |\")\n",
    "    arr += method_generator(second_text_aggregation_method(10, text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5st kõige sagedasemad võtmesõnad on: \n",
      "elurikkus, aasta, linn, inimene, omavalitsus\n",
      "\n",
      "10st kõige sagedasemad võtmesõnad on: \n",
      "elurikkus, aasta, inimene, omavalitsus, linn, taim, sotsiaalne, toetav, hoidmine, artikkel\n",
      "\n"
     ]
    }
   ],
   "source": [
    "positions = [Counter(), Counter()]\n",
    "counts = [Counter(), Counter()]\n",
    "\n",
    "short = [x for x in arr if len(x) == 5]\n",
    "long  = [x for x in arr if len(x) == 10]\n",
    "\n",
    "for i, sizes in enumerate([short, long]):\n",
    "    for case in sizes:\n",
    "        for pos, elem in enumerate(case):\n",
    "            positions[i][elem] += pos\n",
    "            counts[i][elem] += 1\n",
    "\n",
    "total = [Counter(), Counter()]\n",
    "            \n",
    "for i, reps in enumerate([5, 10]):\n",
    "    for item in counts[i]:\n",
    "        total[i][item] = reps - positions[i][item] / counts[i][item]\n",
    "\n",
    "for i, reps in enumerate([5, 10]):\n",
    "    print(str(reps) + \"st kõige sagedasemad võtmesõnad on: \")\n",
    "    print(\", \".join([x[0] for x in total[i].most_common(reps)]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hinnang:\n",
    "\n",
    "---\n",
    "\n",
    "Lõppkokkuvõteks\n",
    " - Sõna \"uuring\" oli huvitav, et tuli esimeseks - anomaalia\n",
    " - Esimene tekstikokkuvõtte meetod and erinevaid tulemusid võrreldes teise kokkuvõtva meetodiga\n",
    " - kõik 3 erinevat võtmesõnade eraldamise algoritmi andsid väga sarnaseid tulemusi jälle\n",
    " - Kokkuvõtvate algoritmide F1 indeksid ei olnud kõige paremad. Aga suuresti võib olla see tingitud sellest, et minu poolt valitud võtmesõnad pole just kõige paremad esindamaks antud teksti\n",
    " - \"elurikkus\", \"aasta\", \"linn\", \"inimene\" ning \"omavalitsus\" on läbivalt top 5 hulgas. Need 5 on kindlalt head võtmesõna\n",
    " - Teine kokkuvõttev meetod tahab väga sõna \"artikkel\" panna võtmesõnaks. Isiklikult ma ei arva, et see peaks olema seal\n",
    " - Päris palju on kordi, kus on näiteks kasutatud \"planeering\", \"planeerimispraktika\" või \"planneerimine\" kasutatud võtmesõnana. Lugeja võib mõtliskleda, et kas võiks iseenesest neid üheks lugeda.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMX96Ps7lrwPLT+kswPV0LP",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Quantitative measures of text complexity for the Independence Day speeches of the Presidents of the Republic of Estonia.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}